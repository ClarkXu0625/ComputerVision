MNIST Classification Class Notes
Some general truth for deep learning training
classify from:  x (3x28x28 image) -(theta)-> y (0~9)
training: f -> (x, y)^N, tune the model f via the training set
Testing: f -> (x) -> y  (**inference**) from unknown set of x
Evaluation: test how the model behave in prepared testing set - how close the prediction to y (accuracy)
    don't contain hyperparameter mart in evaluation

Component: 
1. Model: convolutional neural network for image classification
2. Training set: images and ground-truth labels
3. Loss function: define the gap between predictions and labels.
        Update the model by the loss function, according to the label predicted.
        Take the difference (*loss*) from prediction and ground truth.
4. Optimizer: receive the loss and update the model

General steps for training loop: 
    1. predict result by model, f; 
    2. get the loss by loss function;
    3. tune the model by optimizer;

"batch", hardware is the limitation

architecture:
loss function: cross entropy


separate the inference and evaluation.

